docker ps

# 1ï¸âƒ£ Supprimer les anciens conteneurs interrompus

docker-compose down

# 2ï¸âƒ£ Supprimer les anciennes images partielles

docker image prune -a -f

# 3ï¸âƒ£ Relancer le cluster

docker-compose up -d

docker pull bde2020/hadoop-namenode

# ğŸ§  Projet Big Data Cluster avec Docker

## Hadoop | Spark | Hive | PostgreSQL | Apache NiFi

---

## ğŸ—ï¸ Introduction

Dans le cadre de mon apprentissage en **IngÃ©nierie des DonnÃ©es**, jâ€™ai mis en place un **cluster Big Data** complet basÃ© sur **Docker**.  
Lâ€™objectif est de **simuler un environnement distribuÃ©** similaire Ã  un cluster de production, permettant le **stockage, le traitement et lâ€™analyse de grandes volumÃ©tries de donnÃ©es**.

Ce cluster comprend :

- **Hadoop (HDFS)** pour le stockage distribuÃ© des donnÃ©es.
- **Apache Spark** pour le calcul et le traitement en parallÃ¨le.
- **Apache Hive** pour lâ€™interrogation SQL des donnÃ©es sur HDFS.
- **PostgreSQL** servant de base de mÃ©tadonnÃ©es pour Hive.
- **Apache NiFi** pour lâ€™ingestion et lâ€™orchestration des flux de donnÃ©es.

---

## ğŸ¯ Objectifs du projet

- Mettre en place un **cluster Big Data multi-nÅ“uds** (1 master, 2 slaves) via **Docker Compose**.
- IntÃ©grer les **composants essentiels de lâ€™Ã©cosystÃ¨me Hadoop** (HDFS, Hive, Spark).
- Automatiser le dÃ©ploiement et la configuration des services.
- Faciliter la crÃ©ation dâ€™un **pipeline de donnÃ©es complet** : ingestion â†’ stockage â†’ traitement â†’ visualisation.

---

## ğŸ§© Architecture globale

### ğŸ”¹ Vue dâ€™ensemble du cluster

                 +-----------------------------+
                 |        Master Node          |
                 |-----------------------------|
                 |  Hadoop NameNode            |
                 |  Spark Master               |
                 |  Hive + PostgreSQL Metastore|
                 |  Apache NiFi                |
                 +-----------------------------+
                           |
       -------------------------------------------------
       |                                               |
        +--------------------+ +--------------------+
        | DataNode 1 | | DataNode 2 |
        |--------------------| |--------------------|
        | Hadoop Datanode | | Hadoop Datanode |
        | Spark Worker | | Spark Worker |
        +--------------------+ +--------------------+

---

## âš™ï¸ Technologies utilisÃ©es

| Outil              | RÃ´le                 | Description                                                                                    |
| ------------------ | -------------------- | ---------------------------------------------------------------------------------------------- |
| **Hadoop (HDFS)**  | Stockage distribuÃ©   | DÃ©coupe et rÃ©partit les fichiers sur plusieurs nÅ“uds pour tolÃ©rance aux pannes et scalabilitÃ©. |
| **Spark**          | Traitement distribuÃ© | ExÃ©cute les calculs Ã  grande Ã©chelle en mÃ©moire, plus rapide que MapReduce.                    |
| **Hive**           | Interrogation SQL    | Fournit une interface SQL au-dessus dâ€™HDFS pour manipuler les donnÃ©es facilement.              |
| **PostgreSQL**     | MÃ©tastore Hive       | Contient les mÃ©tadonnÃ©es des tables Hive (schÃ©mas, partitions, etc.).                          |
| **NiFi**           | Ingestion de donnÃ©es | Permet de capturer, transformer et charger les donnÃ©es depuis diverses sources.                |
| **Docker Compose** | Orchestration        | Permet de lancer et connecter tous les services avec une seule commande.                       |

---

## ğŸ§± Structure du cluster Docker

### ğŸ“ Arborescence du projet


 docker exec -it namenode bash

docker exec -it namenode bash
root@d0569a8c43eb:/# hdfs dfs -mkdir -p /data2/test
root@d0569a8c43eb:/# hdfs dfs -chown nifi:supergroup /data2/test
root@d0569a8c43eb:/# hdfs dfs -ls /data2
Found 1 items
drwxr-xr-x   - nifi supergroup          0 2025-11-08 11:49 /data2/test
root@d0569a8c43eb:/# hdfs dfs -ls /data2/test/temperatures
ls: `/data2/test/temperatures': No such file or directory
root@d0569a8c43eb:/# hdfs dfs -ls /data2/test
root@d0569a8c43eb:/# hdfs dfs -ls /data2/test
Found 1 items
drwxr-xr-x   - nifi supergroup          0 2025-11-08 12:02 /data2/test/temperatures
root@d0569a8c43eb:/# exit
exit


version: "3.9"

services:
  # =========================
  # ğŸ—„ï¸ Base de donnÃ©es PostgreSQL pour Hive Metastore
  # =========================
  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore
    ports:
      - "5432:5432"
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    networks:
      - bigdata-net

  # =========================
  # ğŸ§± Hadoop - NameNode
  # =========================
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=bigdata
    ports:
      - "8020:8020"
      - "9870:9870"
      - "9000:9000"
    volumes:
      - ./hadoop:/opt/hadoop/etc/hadoop
      - namenode:/hadoop/dfs/name
    env_file:
      - ./hadoop/hadoop.env
    networks:
      - bigdata-net

  # =========================
  # ğŸ§± Hadoop - DataNodes
  # =========================
  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    ports:
      - "9864:9864"
    volumes:
      - datanode1:/hadoop/dfs/data
    networks:
      - bigdata-net
    depends_on:
      - namenode

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    ports:
      - "9865:9864"
    volumes:
      - datanode2:/hadoop/dfs/data
    networks:
      - bigdata-net
    depends_on:
      - namenode

  # =========================
  # âš¡ Spark Master
  # =========================
  spark:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark
    environment:
      - SPARK_MODE=master
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./spark:/spark/conf
    networks:
      - bigdata-net
    depends_on:
      - namenode
      - datanode1
      - datanode2

  # =========================
  # âš¡ Spark Worker
  # =========================
  spark-worker1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker1
    environment:
      - SPARK_MASTER=spark://spark:7077
    ports:
      - "8081:8081"
    networks:
      - bigdata-net
    depends_on:
      - spark

  # =========================
  # ğŸ Hive (Metastore + HiveServer2)
  # =========================
  hive:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive
    environment:
      HIVE_METASTORE_DB_HOST: postgres
      HIVE_METASTORE_DB_NAME: metastore
      HIVE_METASTORE_DB_USER: hive
      HIVE_METASTORE_DB_PASS: hive
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    ports:
      - "10000:10000"
      - "10002:10002"
    depends_on:
      - namenode
      - datanode1
      - datanode2
      - postgres
    networks:
      - bigdata-net
    command: >
      bash -c "
        echo 'ğŸ•’ Attente de HDFS et PostgreSQL...';
        until nc -z namenode 9000 && nc -z postgres 5432; do
          echo 'â³ En attente de HDFS ou Postgres...';
          sleep 5;
        done;
        echo 'âœ… HDFS et Postgres prÃªts.';

        echo 'ğŸ”§ Initialisation du schÃ©ma Hive (si nÃ©cessaire)...';
        schematool -dbType postgres -initSchema || echo 'âœ… SchÃ©ma dÃ©jÃ  initialisÃ©.';

        echo 'ğŸš€ DÃ©marrage du Metastore Hive...';
        nohup /opt/hive/bin/hive --service metastore > /var/log/metastore.log 2>&1 &

        echo 'ğŸš€ DÃ©marrage de HiveServer2...';
        exec /opt/hive/bin/hive --service hiveserver2 --hiveconf hive.root.logger=INFO,console
      "

  # =========================
  # ğŸ”„ Apache NiFi
  # =========================
  nifi:
    image: apache/nifi:1.27.0
    container_name: nifi
    ports:
      - "8089:8080"
    environment:
      - NIFI_WEB_HTTP_PORT=8080
    volumes:
      - ./data2:/data2
      - ./hadoop:/opt/hadoop/etc/hadoop
    networks:
      - bigdata-net
    depends_on:
      - hive
      - spark

# =========================
# ğŸ”— RÃ©seau et volumes
# =========================
networks:
  bigdata-net:

volumes:
  namenode:
  datanode1:
  datanode2:




--------------------------------------------------------------------

















version: "3.9"

services:
  # =========================
  # ğŸ—„ï¸ Base de donnÃ©es PostgreSQL pour Hive Metastore
  # =========================
  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore
    ports:
      - "5432:5432"
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    networks:
      - bigdata-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive -d metastore"]
      interval: 10s
      timeout: 5s
      retries: 5

  # =========================
  # ğŸ§± Hadoop - NameNode
  # =========================
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=bigdata
    ports:
      - "8020:8020"
      - "9870:9870"
      - "9000:9000"
    volumes:
      - ./hadoop:/opt/hadoop/etc/hadoop
      - namenode:/hadoop/dfs/name
    env_file:
      - ./hadoop/hadoop.env
    networks:
      - bigdata-net
    healthcheck:
      test: ["CMD", "hdfs", "dfsadmin", "-safemode", "get"]
      interval: 10s
      timeout: 10s
      retries: 10

  # =========================
  # ğŸ§± Hadoop - DataNodes
  # =========================
  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    ports:
      - "9864:9864"
    volumes:
      - datanode1:/hadoop/dfs/data
    networks:
      - bigdata-net
    depends_on:
      - namenode

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    ports:
      - "9865:9864"
    volumes:
      - datanode2:/hadoop/dfs/data
    networks:
      - bigdata-net
    depends_on:
      - namenode

  # =========================
  # âš¡ Spark Master
  # =========================
  spark:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark
    environment:
      - SPARK_MODE=master
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./spark:/spark/conf
    networks:
      - bigdata-net
    depends_on:
      - namenode

  # =========================
  # âš¡ Spark Worker
  # =========================
  spark-worker1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker1
    environment:
      - SPARK_MASTER=spark://spark:7077
    ports:
      - "8081:8081"
    networks:
      - bigdata-net
    depends_on:
      - spark

  # =========================
  # ğŸ Hive (Metastore + HiveServer2)
  # =========================
  hive:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive
    environment:
      HIVE_METASTORE_DB_HOST: postgres
      HIVE_METASTORE_DB_NAME: metastore
      HIVE_METASTORE_DB_USER: hive
      HIVE_METASTORE_DB_PASS: hive
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      HIVE_SITE_CONF_javax_jdo_option_ConnectionURL: jdbc:postgresql://postgres:5432/metastore
      HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName: org.postgresql.Driver
      HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName: hive
      HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword: hive
    ports:
      - "10000:10000"
      - "10002:10002"
      - "9083:9083"
    depends_on:
      postgres:
        condition: service_healthy
      namenode:
        condition: service_healthy
    networks:
      - bigdata-net
    volumes:
      - ./scripts:/scripts
    command: >
      bash -c "
        echo 'ğŸ•’ Attente de HDFS et PostgreSQL...';
        until nc -z namenode 9000 && nc -z postgres 5432; do
          echo 'â³ En attente...';
          sleep 5;
        done;
        echo 'âœ… HDFS et Postgres prÃªts.';
        
        echo 'ğŸ”§ Initialisation du schÃ©ma Hive...';
        /opt/hive/bin/schematool -dbType postgres -initSchema -verbose || echo 'â„¹ï¸  SchÃ©ma peut-Ãªtre dÃ©jÃ  initialisÃ©';
        
        echo 'ğŸš€ DÃ©marrage du Metastore Hive en arriÃ¨re-plan...';
        /opt/hive/bin/hive --service metastore &
        
        echo 'â³ Attente du dÃ©marrage du Metastore...';
        sleep 15;
        
        echo 'ğŸš€ DÃ©marrage de HiveServer2...';
        exec /opt/hive/bin/hiveserver2
      "

  # =========================
  # ğŸ”„ Apache NiFi
  # =========================
  nifi:
    image: apache/nifi:1.27.0
    container_name: nifi
    ports:
      - "8089:8080"
    environment:
      - NIFI_WEB_HTTP_PORT=8080
    volumes:
      - ./data2:/data2
      - ./hadoop:/opt/hadoop/etc/hadoop
    networks:
      - bigdata-net
    depends_on:
      - hive

networks:
  bigdata-net:

volumes:
  namenode:
  datanode1:
  datanode2:
