# spark-defaults.conf : Paramètres par défaut de Spark.
# Ici, on configure Spark pour communiquer avec le cluster distribué.

# Adresse du Spark Master
spark.master spark://spark-master:7077

# Activation des journaux d'événements Spark (utile pour le debugging)
spark.eventLog.enabled true

# Répertoire HDFS où Spark stocke ses logs
spark.eventLog.dir hdfs://namenode:8020/spark-logs
spark.history.fs.logDirectory hdfs://namenode:8020/spark-logs

# Permet d'utiliser les tables Hive (metastore partagé)
spark.sql.catalogImplementation hive

# Adresse du ResourceManager YARN
spark.hadoop.yarn.resourcemanager.address namenode:8032

# Ressources allouées pour le driver et les executors Spark
spark.driver.memory 1g
spark.executor.memory 1g
